{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of who-wrote-this.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw6VsDhUun5n",
        "colab_type": "text"
      },
      "source": [
        "# Who wrote this : a framework for French novelist identification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vARA24RjBr3f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "577ed933-50e9-4d4d-8a75-bbc07d4ceaef"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd 'drive/My Drive/who-wrote-this/'"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: 'drive/My Drive/who-wrote-this/'\n",
            "/content/drive/My Drive/who-wrote-this\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rR8SIQ_ks-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install --upgrade gensim\n",
        "# !pip install unidecode\n",
        "# !pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liRJXYg6Cv2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "import unidecode\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import f1_score\n",
        "import gensim\n",
        "from gensim.models import Doc2Vec, FastText\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoModel, AutoTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUk_HtdMolVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of available cores for parallel computing\n",
        "N_CORES = cpu_count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPykVwoCzxbu",
        "colab_type": "text"
      },
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i0eCW7-DDDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import train data\n",
        "train_df = pd.read_csv('data/corpus_train.csv', sep='|')\n",
        "train_df = train_df.sample(frac=1).reset_index(drop=True) # Shuffle\n",
        "X_train = train_df['paragraph'].values\n",
        "y_labels_train = train_df['author'].values\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_labels_train)\n",
        "N_CLASSES = len(np.unique(y_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPnN3eBy3slZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import test data and build validation dataset\n",
        "test_df = pd.read_csv('data/corpus_test.csv', sep='|')\n",
        "test_df = test_df.sample(frac=1).reset_index(drop=True) # Shuffle\n",
        "X_val, X_test, y_val, y_test = train_test_split(test_df['paragraph'].values,\n",
        "                                                test_df['author'].values,\n",
        "                                                test_size=0.5, random_state=42)\n",
        "y_val = le.transform(y_val)\n",
        "y_test = le.transform(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSOiEE20z1o4",
        "colab_type": "text"
      },
      "source": [
        "## Baseline : TF-IDF + Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOwk-ES44rEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ML pipeline : TF-IDF + SVM classifier\n",
        "\n",
        "tfidf_vecto = TfidfVectorizer()\n",
        "clf = LogisticRegression(max_iter=10000)\n",
        "\n",
        "tfidf_pipeline = Pipeline([\n",
        "                           ('tf-idf', tfidf_vecto),\n",
        "                           ('SVC', clf)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KlP_Iw2oth4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Keep sklearn preprocessing pipeline for later\n",
        "preprocessor = tfidf_vecto.build_analyzer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ML1EXEK2kA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing + training\n",
        "tfidf_pipeline = tfidf_pipeline.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdyNC-ok3qxy",
        "colab_type": "code",
        "outputId": "09813d09-2b53-407e-c7f1-c07cffe08ed9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Compute predictions and validation score\n",
        "y_val_pred_tfidf = tfidf_pipeline.predict(X_val)\n",
        "tfidf_val_score = f1_score(y_val, y_val_pred_tfidf, average='micro')\n",
        "print('F1 score on validation set with TF-IDF :', \n",
        "      tfidf_val_score.round(2))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score on validation set with TF-IDF : 0.52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g88tOQuBz792",
        "colab_type": "text"
      },
      "source": [
        "## Averaging of FastText pre-trained word vectors + Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAYXl07XADFH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8b5ff926-2760-4f6f-dbab-3d7eb7bdd1f2"
      },
      "source": [
        "# Import Fasttext French word vectors\n",
        "fasttext = FastText.load_fasttext_format('models/fasttext.fr.300.bin')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r151opCRr8B1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_to_wv_fasttext(text):\n",
        "    \"\"\"Compute average of FastText's word vectors for a given text.\"\"\"\n",
        "    if text:\n",
        "        tokens = preprocessor(text)\n",
        "        wv_mat = np.zeros((len(tokens), fasttext.vector_size))\n",
        "        for i, tok in enumerate(tokens):\n",
        "            try:\n",
        "                wv_mat[i] = fasttext.wv[tok]\n",
        "            except KeyError:\n",
        "                pass\n",
        "        text_vec = wv_mat.mean(axis=0)\n",
        "    else:\n",
        "        text_vec = np.zeros(fasttext.vector_size)\n",
        "    return text_vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRfHhI-LuzPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_corpus_fasttext(corpus):\n",
        "    \"\"\"Parallelize preprocessing and document vectors computation.\"\"\"\n",
        "    with Pool(N_CORES) as p:\n",
        "        corpus_prepro = p.map(text_to_wv_fasttext, list(corpus))\n",
        "    return np.array(corpus_prepro)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV7aqaByn1B3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextToWV(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Enable to use preprocessing function in a sklearn pipeline.\"\"\"\n",
        "    def __init__(self, preprocessor):\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return(self)\n",
        "\n",
        "    def transform(self, X):\n",
        "        return self.preprocessor(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sg3YBbgJ0ZPm",
        "colab": {}
      },
      "source": [
        "# Prediction pipeline\n",
        "clf = LogisticRegression(max_iter=10000)\n",
        "fasttext_pipeline = Pipeline([\n",
        "                              ('fasttext_average', TextToWV(preprocess_corpus_fasttext)),\n",
        "                              ('SVC', clf)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTvo5xwMob3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing + training\n",
        "fasttext_pipeline = fasttext_pipeline.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIVGLvOm0pY_",
        "colab_type": "code",
        "outputId": "cebc2815-738d-4540-c7c9-502037ae8fe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Compute predictions and test score\n",
        "y_val_pred_fasttext = fasttext_pipeline.predict(X_val)\n",
        "val_score_fasttext = f1_score(y_val, y_val_pred_fasttext, average='micro')\n",
        "print('F1 score on test set with pre-trained FastText + averaging :',\n",
        "      val_score_fasttext.round(2))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score on test set with pre-trained FastText + averaging : 0.35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL0DTOLbvYq1",
        "colab_type": "text"
      },
      "source": [
        "## Doc2Vec + Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztQM6FneveCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_d2v_corpus(corpus, tokens_only=False):\n",
        "    \"\"\"Tokenize and build corpus as expected by Gensim Doc2Vec class.\"\"\"\n",
        "    corpus_tokenized = []\n",
        "    for i, text in enumerate(corpus):\n",
        "        tokens = preprocessor(text)\n",
        "        if tokens_only:\n",
        "            corpus_tokenized.append(tokens)\n",
        "        else:\n",
        "            corpus_tokenized.append(TaggedDocument(tokens, [i]))\n",
        "    return corpus_tokenized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krsC1SYh5ACx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Format train and validation corpus as required by Doc2Vec\n",
        "corpus_train_d2v = build_d2v_corpus(X_train, tokens_only=False)\n",
        "corpus_val_d2v = build_d2v_corpus(X_val, tokens_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryhxTy1C5wwy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "5e2dea49-9f5b-4424-e01a-715b63e5e786"
      },
      "source": [
        "# Train Doc2Vec model\n",
        "model = Doc2Vec(vector_size=50, min_count=2, epochs=10, workers=N_CORES)\n",
        "model.build_vocab(corpus_train_d2v)\n",
        "model.train(corpus_train_d2v, total_examples=model.corpus_count, \n",
        "            epochs=model.epochs)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-34c4f06fc4c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_train_d2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.train(corpus_train_d2v, total_examples=model.corpus_count, \n\u001b[0;32m----> 4\u001b[0;31m             epochs=model.epochs)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, documents, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pLoVbMK8_DI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute document vectors on train and validation sets\n",
        "X_train_d2v = np.array([model.infer_vector(doc.words) for doc in corpus_train_d2v])\n",
        "X_val_d2v = np.array([model.infer_vector(doc) for doc in corpus_val_d2v])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe3LTThCAICg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute predictions and validation score\n",
        "clf = LogisticRegression(max_iter=10000)\n",
        "clf.fit(X_train_d2v, y_train)\n",
        "y_val_pred_d2v = clf.predict(X_val_d2v)\n",
        "val_score_d2v = f1_score(y_val, y_val_pred_d2v, average='micro')\n",
        "print('F1 score on validation set with Doc2Vec :', \n",
        "      val_score_d2v.round(2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUqdN5M90AQV",
        "colab_type": "text"
      },
      "source": [
        "## CamemBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6kCmiR6WQWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BERT_MODEL_NAME = 'camembert-base'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6W9sKtFRXbL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CorpusToTorchDataset(Dataset):\n",
        "    \"\"\"Convert corpus to tensors of token indices in CamemBERT vocabulary.\"\"\"\n",
        "    def __init__(self, corpus, labels, model_name=BERT_MODEL_NAME, maxlen=100):\n",
        "        self.corpus = corpus\n",
        "        self.labels = labels\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.corpus)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # Select instance\n",
        "        sentence = self.corpus[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        # Preprocess data as required by BERT models\n",
        "        tokens = self.tokenizer.tokenize(sentence)\n",
        "        bos_token = self.tokenizer.cls_token\n",
        "        eos_token = self.tokenizer.sep_token\n",
        "        pad_token = self.tokenizer.pad_token\n",
        "        # Insert CLS and SEP tokens at beginning and end of sentence\n",
        "        tokens = [bos_token] + tokens + [eos_token]\n",
        "        if len(tokens) < self.maxlen:\n",
        "            # If sentence is shorter than maxlen, pad sentence using special \n",
        "            # padding token\n",
        "            tokens = tokens + [pad_token for _ in range(self.maxlen - len(tokens))]\n",
        "        else:\n",
        "            # Cut the sentence if it is longer than maxlen\n",
        "            tokens = tokens[:self.maxlen-1] + [eos_token]\n",
        "\n",
        "        # Convert tokens to tensor of indices in CamemBERT vocabulary\n",
        "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        tokens_ids_tensor = torch.tensor(tokens_ids)\n",
        "        # Get attention mask to distinguish padding tokens from actual tokens\n",
        "        pad_token_id = self.tokenizer.convert_tokens_to_ids(pad_token)\n",
        "        attn_mask = (tokens_ids_tensor != pad_token_id).long()\n",
        "\n",
        "        return tokens_ids_tensor, attn_mask, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDnF51VmVtHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create instances of training and validation dataloaders\n",
        "train_set = CorpusToTorchDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_set, batch_size = 12, num_workers = N_CORES)\n",
        "\n",
        "val_set = CorpusToTorchDataset(X_val, y_val)\n",
        "val_loader = DataLoader(val_set, batch_size = 12, num_workers = N_CORES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ydN8F8gOUI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CamemBERTClassifier(nn.Module):\n",
        "    \"\"\"Perform fine-tuning and classification using CamemBERT.\"\"\"\n",
        "    def __init__(self, pretrained_model_name=BERT_MODEL_NAME):\n",
        "        super(CamemBERTClassifier, self).__init__()\n",
        "        # Load CamemBERT\n",
        "        self.encoder = AutoModel.from_pretrained(pretrained_model_name)\n",
        "        # Add an extra dense layer to perform classification\n",
        "        self.cls_layer = nn.Linear(self.encoder.pooler.dense.out_features, N_CLASSES)\n",
        "\n",
        "    def forward(self, seq, attn_masks):\n",
        "        # Feed input to BERT model to obtain contextualized representations\n",
        "        cont_reps, _ = self.encoder(seq, attention_mask = attn_masks)\n",
        "        # Get representation of [CLS] head\n",
        "        cls_rep = cont_reps[:, 0]\n",
        "        # Feed document representation to the classifying layer\n",
        "        logits = self.cls_layer(cls_rep)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4A_4QjGOUP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate CamemBERT classifier model\n",
        "camembert_clf = CamemBERTClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f14mz-vhsuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define loss and optimizer\n",
        "criterion = CrossEntropyLoss()\n",
        "opti = Adam(camembert_clf.parameters(), lr = 3e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZUQxW1biwti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, criterion, opti, train_loader, val_loader, max_eps=10, \n",
        "          gpu=True, print_every=100, validate_every=1, break_training_after=None):\n",
        "    if gpu:\n",
        "        model = model.to(\"cuda\")\n",
        "    batch_accuracies = []\n",
        "    for ep in range(max_eps):\n",
        "        \n",
        "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
        "            # clear gradients\n",
        "            opti.zero_grad()  \n",
        "            # transfer tensors to GPU\n",
        "            if gpu:\n",
        "                seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
        "            # compute logits\n",
        "            logits = model(seq, attn_masks)\n",
        "            # compute loss\n",
        "            loss = criterion(logits, labels)\n",
        "            # backpropagation\n",
        "            loss.backward()\n",
        "            # optimization step\n",
        "            opti.step()\n",
        "\n",
        "            batch_acc = torch.sum(torch.argmax(logits, 1) == labels) / float(labels.size(0))\n",
        "            batch_accuracies.append(batch_acc.item())\n",
        "\n",
        "            if (it + 1) % print_every == 0:\n",
        "                mean_batch_acc = np.array(batch_accuracies).mean()\n",
        "                print(\"Iteration {} of epoch {} complete. Mean batch accuracy : {}\".format(it+1, ep+1, mean_batch_acc))\n",
        "\n",
        "        if ep % validate_every == 0:\n",
        "            # evaluation on the validation set\n",
        "            n_batch_validation = 0\n",
        "            loss_validation = 0\n",
        "            accuracy_validation = 0\n",
        "            for it, (seq, attn_masks, labels) in enumerate(val_loader):\n",
        "                if gpu:\n",
        "                    seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
        "                # compute logits\n",
        "                logits_val = model(seq, attn_masks)\n",
        "                n_batch_validation+=1\n",
        "                # compute loss and accuracy\n",
        "                _loss = float(criterion(logits_val, labels))\n",
        "                loss_validation += _loss\n",
        "                _accu = torch.sum(torch.argmax(logits, 1) == labels) / float(labels.size(0))\n",
        "                accuracy_validation += _accu\n",
        "\n",
        "            print(\"EVALUATION Validation set : mean loss {} n mean accuracy {}\".format(loss_validation/n_batch_validation, accuracy_validation/n_batch_validation))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nfxNPWenHb-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dac1b21f-51f5-4faa-8fdf-611b9438b203"
      },
      "source": [
        "train(camembert_clf, criterion, opti, train_loader, val_loader,\n",
        "      max_eps=5, gpu=True, print_every=10, validate_every=1)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 10 of epoch 1 complete. Mean batch accuracy : 0.2750000074505806\n",
            "Iteration 20 of epoch 1 complete. Mean batch accuracy : 0.2500000063329935\n",
            "Iteration 30 of epoch 1 complete. Mean batch accuracy : 0.2750000057121118\n",
            "Iteration 40 of epoch 1 complete. Mean batch accuracy : 0.27291667331010105\n",
            "Iteration 50 of epoch 1 complete. Mean batch accuracy : 0.27500000640749933\n",
            "Iteration 60 of epoch 1 complete. Mean batch accuracy : 0.2875000076989333\n",
            "Iteration 70 of epoch 1 complete. Mean batch accuracy : 0.2845238168324743\n",
            "Iteration 80 of epoch 1 complete. Mean batch accuracy : 0.2802083405666053\n",
            "Iteration 90 of epoch 1 complete. Mean batch accuracy : 0.27962963672147856\n",
            "Iteration 100 of epoch 1 complete. Mean batch accuracy : 0.27583334036171436\n",
            "Iteration 110 of epoch 1 complete. Mean batch accuracy : 0.2856060678985986\n",
            "Iteration 120 of epoch 1 complete. Mean batch accuracy : 0.2854166740551591\n",
            "Iteration 130 of epoch 1 complete. Mean batch accuracy : 0.2871794948211083\n",
            "Iteration 140 of epoch 1 complete. Mean batch accuracy : 0.28690476939082143\n",
            "Iteration 150 of epoch 1 complete. Mean batch accuracy : 0.2894444519281387\n",
            "Iteration 160 of epoch 1 complete. Mean batch accuracy : 0.28906250749714674\n",
            "Iteration 170 of epoch 1 complete. Mean batch accuracy : 0.2852941249661586\n",
            "Iteration 180 of epoch 1 complete. Mean batch accuracy : 0.28472222946584225\n",
            "Iteration 190 of epoch 1 complete. Mean batch accuracy : 0.2828947440574044\n",
            "Iteration 200 of epoch 1 complete. Mean batch accuracy : 0.28125000711530446\n",
            "Iteration 210 of epoch 1 complete. Mean batch accuracy : 0.28134921352778164\n",
            "Iteration 220 of epoch 1 complete. Mean batch accuracy : 0.2829545528373935\n",
            "Iteration 230 of epoch 1 complete. Mean batch accuracy : 0.28297102188934453\n",
            "Iteration 240 of epoch 1 complete. Mean batch accuracy : 0.28402778496965764\n",
            "Iteration 250 of epoch 1 complete. Mean batch accuracy : 0.2853333403766155\n",
            "Iteration 260 of epoch 1 complete. Mean batch accuracy : 0.2881410328241495\n",
            "Iteration 270 of epoch 1 complete. Mean batch accuracy : 0.288271612232482\n",
            "Iteration 280 of epoch 1 complete. Mean batch accuracy : 0.2872023882610457\n",
            "Iteration 290 of epoch 1 complete. Mean batch accuracy : 0.2873563291183833\n",
            "Iteration 300 of epoch 1 complete. Mean batch accuracy : 0.2875000074505806\n",
            "Iteration 310 of epoch 1 complete. Mean batch accuracy : 0.2884408677056913\n",
            "Iteration 320 of epoch 1 complete. Mean batch accuracy : 0.2890625073341653\n",
            "Iteration 330 of epoch 1 complete. Mean batch accuracy : 0.29040404774925926\n",
            "Iteration 340 of epoch 1 complete. Mean batch accuracy : 0.289215693644741\n",
            "Iteration 350 of epoch 1 complete. Mean batch accuracy : 0.2904761978558132\n",
            "Iteration 360 of epoch 1 complete. Mean batch accuracy : 0.28888889615320495\n",
            "Iteration 370 of epoch 1 complete. Mean batch accuracy : 0.2894144215293833\n",
            "Iteration 380 of epoch 1 complete. Mean batch accuracy : 0.28991228779287714\n",
            "Iteration 390 of epoch 1 complete. Mean batch accuracy : 0.29059829759674194\n",
            "Iteration 400 of epoch 1 complete. Mean batch accuracy : 0.2887500069476664\n",
            "Iteration 410 of epoch 1 complete. Mean batch accuracy : 0.2908536655146901\n",
            "Iteration 420 of epoch 1 complete. Mean batch accuracy : 0.2912698482473691\n",
            "Iteration 430 of epoch 1 complete. Mean batch accuracy : 0.29205427055442057\n",
            "Iteration 440 of epoch 1 complete. Mean batch accuracy : 0.29204546145417476\n",
            "Iteration 450 of epoch 1 complete. Mean batch accuracy : 0.29240741435024475\n",
            "Iteration 460 of epoch 1 complete. Mean batch accuracy : 0.29257247081917265\n",
            "Iteration 470 of epoch 1 complete. Mean batch accuracy : 0.2930851134531041\n",
            "Iteration 480 of epoch 1 complete. Mean batch accuracy : 0.29322917376024027\n",
            "Iteration 490 of epoch 1 complete. Mean batch accuracy : 0.29302721797507636\n",
            "Iteration 500 of epoch 1 complete. Mean batch accuracy : 0.2930000071078539\n",
            "Iteration 510 of epoch 1 complete. Mean batch accuracy : 0.2949346477056251\n",
            "Iteration 520 of epoch 1 complete. Mean batch accuracy : 0.2951923148133434\n",
            "Iteration 530 of epoch 1 complete. Mean batch accuracy : 0.2957547241646164\n",
            "Iteration 540 of epoch 1 complete. Mean batch accuracy : 0.296913587467538\n",
            "Iteration 550 of epoch 1 complete. Mean batch accuracy : 0.2969697042215954\n",
            "Iteration 560 of epoch 1 complete. Mean batch accuracy : 0.2977678645136101\n",
            "Iteration 570 of epoch 1 complete. Mean batch accuracy : 0.2985380190898452\n",
            "Iteration 580 of epoch 1 complete. Mean batch accuracy : 0.29770115675597353\n",
            "Iteration 590 of epoch 1 complete. Mean batch accuracy : 0.2981638492037684\n",
            "Iteration 600 of epoch 1 complete. Mean batch accuracy : 0.29777778512487807\n",
            "Iteration 610 of epoch 1 complete. Mean batch accuracy : 0.29740437886021176\n",
            "Iteration 620 of epoch 1 complete. Mean batch accuracy : 0.29825269551046435\n",
            "Iteration 630 of epoch 1 complete. Mean batch accuracy : 0.298148155460755\n",
            "Iteration 640 of epoch 1 complete. Mean batch accuracy : 0.29856771564809603\n",
            "Iteration 650 of epoch 1 complete. Mean batch accuracy : 0.298846161205035\n",
            "Iteration 660 of epoch 1 complete. Mean batch accuracy : 0.29974748214537444\n",
            "Iteration 670 of epoch 1 complete. Mean batch accuracy : 0.3002487636324185\n",
            "Iteration 680 of epoch 1 complete. Mean batch accuracy : 0.30000000737388344\n",
            "Iteration 690 of epoch 1 complete. Mean batch accuracy : 0.29951691555156224\n",
            "Iteration 700 of epoch 1 complete. Mean batch accuracy : 0.29976191213088377\n",
            "Iteration 710 of epoch 1 complete. Mean batch accuracy : 0.2998826364711137\n",
            "Iteration 720 of epoch 1 complete. Mean batch accuracy : 0.30057871106805073\n",
            "Iteration 730 of epoch 1 complete. Mean batch accuracy : 0.3004566283666924\n",
            "Iteration 740 of epoch 1 complete. Mean batch accuracy : 0.30067568306584613\n",
            "Iteration 750 of epoch 1 complete. Mean batch accuracy : 0.30111111856500306\n",
            "Iteration 760 of epoch 1 complete. Mean batch accuracy : 0.3007675513331043\n",
            "Iteration 770 of epoch 1 complete. Mean batch accuracy : 0.30043290793508676\n",
            "Iteration 780 of epoch 1 complete. Mean batch accuracy : 0.3001068450988103\n",
            "Iteration 790 of epoch 1 complete. Mean batch accuracy : 0.3002109779681586\n",
            "Iteration 800 of epoch 1 complete. Mean batch accuracy : 0.3001041741576046\n",
            "Iteration 810 of epoch 1 complete. Mean batch accuracy : 0.3010288141483878\n",
            "Iteration 820 of epoch 1 complete. Mean batch accuracy : 0.30030488558113577\n",
            "Iteration 830 of epoch 1 complete. Mean batch accuracy : 0.3014056301260569\n",
            "Iteration 840 of epoch 1 complete. Mean batch accuracy : 0.30119048381845154\n",
            "Iteration 850 of epoch 1 complete. Mean batch accuracy : 0.30107843902181175\n",
            "Iteration 860 of epoch 1 complete. Mean batch accuracy : 0.3014534960046064\n",
            "Iteration 870 of epoch 1 complete. Mean batch accuracy : 0.3014367892567454\n",
            "Iteration 880 of epoch 1 complete. Mean batch accuracy : 0.30170455309139055\n",
            "Iteration 890 of epoch 1 complete. Mean batch accuracy : 0.3007490712605166\n",
            "Iteration 900 of epoch 1 complete. Mean batch accuracy : 0.30092593356966973\n",
            "Iteration 910 of epoch 1 complete. Mean batch accuracy : 0.30036630799154657\n",
            "Iteration 920 of epoch 1 complete. Mean batch accuracy : 0.30108696415858427\n",
            "Iteration 930 of epoch 1 complete. Mean batch accuracy : 0.30107527645204657\n",
            "Iteration 940 of epoch 1 complete. Mean batch accuracy : 0.3013297948510723\n",
            "Iteration 950 of epoch 1 complete. Mean batch accuracy : 0.3016666743080867\n",
            "Iteration 960 of epoch 1 complete. Mean batch accuracy : 0.3013888964895159\n",
            "Iteration 970 of epoch 1 complete. Mean batch accuracy : 0.3013745780227725\n",
            "Iteration 980 of epoch 1 complete. Mean batch accuracy : 0.30161565383326033\n",
            "Iteration 990 of epoch 1 complete. Mean batch accuracy : 0.30227273483619543\n",
            "Iteration 1000 of epoch 1 complete. Mean batch accuracy : 0.30225000757724046\n",
            "Iteration 1010 of epoch 1 complete. Mean batch accuracy : 0.3023927468769621\n",
            "Iteration 1020 of epoch 1 complete. Mean batch accuracy : 0.3019607919323094\n",
            "Iteration 1030 of epoch 1 complete. Mean batch accuracy : 0.3024271920829722\n",
            "Iteration 1040 of epoch 1 complete. Mean batch accuracy : 0.3020833409462984\n",
            "Iteration 1050 of epoch 1 complete. Mean batch accuracy : 0.3026190552541188\n",
            "Iteration 1060 of epoch 1 complete. Mean batch accuracy : 0.30275157996506064\n",
            "Iteration 1070 of epoch 1 complete. Mean batch accuracy : 0.30280374597723236\n",
            "Iteration 1080 of epoch 1 complete. Mean batch accuracy : 0.30231482247235597\n",
            "Iteration 1090 of epoch 1 complete. Mean batch accuracy : 0.302599396057632\n",
            "Iteration 1100 of epoch 1 complete. Mean batch accuracy : 0.3026515228233554\n",
            "Iteration 1110 of epoch 1 complete. Mean batch accuracy : 0.30307808577477396\n",
            "Iteration 1120 of epoch 1 complete. Mean batch accuracy : 0.30305060290598446\n",
            "Iteration 1130 of epoch 1 complete. Mean batch accuracy : 0.3027286212133095\n",
            "Iteration 1140 of epoch 1 complete. Mean batch accuracy : 0.30219299007664646\n",
            "Iteration 1150 of epoch 1 complete. Mean batch accuracy : 0.3021014568663162\n",
            "Iteration 1160 of epoch 1 complete. Mean batch accuracy : 0.30265805355819136\n",
            "Iteration 1170 of epoch 1 complete. Mean batch accuracy : 0.30299146060123405\n",
            "Iteration 1180 of epoch 1 complete. Mean batch accuracy : 0.3026836234129081\n",
            "Iteration 1190 of epoch 1 complete. Mean batch accuracy : 0.3025910439924533\n",
            "Iteration 1200 of epoch 1 complete. Mean batch accuracy : 0.30270834093913435\n",
            "Iteration 1210 of epoch 1 complete. Mean batch accuracy : 0.3028236990809933\n",
            "Iteration 1220 of epoch 1 complete. Mean batch accuracy : 0.3029371660935586\n",
            "Iteration 1230 of epoch 1 complete. Mean batch accuracy : 0.3027777854080607\n",
            "Iteration 1240 of epoch 1 complete. Mean batch accuracy : 0.30302420119724927\n",
            "Iteration 1250 of epoch 1 complete. Mean batch accuracy : 0.3026666743040085\n",
            "Iteration 1260 of epoch 1 complete. Mean batch accuracy : 0.3031084732995147\n",
            "Iteration 1270 of epoch 1 complete. Mean batch accuracy : 0.30301838040821194\n",
            "Iteration 1280 of epoch 1 complete. Mean batch accuracy : 0.30305990354390816\n",
            "Iteration 1290 of epoch 1 complete. Mean batch accuracy : 0.30290698444889497\n",
            "Iteration 1300 of epoch 1 complete. Mean batch accuracy : 0.30262821283478003\n",
            "Iteration 1310 of epoch 1 complete. Mean batch accuracy : 0.302926216337062\n",
            "Iteration 1320 of epoch 1 complete. Mean batch accuracy : 0.3028409167785536\n",
            "Iteration 1330 of epoch 1 complete. Mean batch accuracy : 0.30300752649405843\n",
            "Iteration 1340 of epoch 1 complete. Mean batch accuracy : 0.30354478384243017\n",
            "Iteration 1350 of epoch 1 complete. Mean batch accuracy : 0.3036419830278114\n",
            "Iteration 1360 of epoch 1 complete. Mean batch accuracy : 0.3031250077135423\n",
            "Iteration 1370 of epoch 1 complete. Mean batch accuracy : 0.3033455065349593\n",
            "Iteration 1380 of epoch 1 complete. Mean batch accuracy : 0.3034420367330313\n",
            "Iteration 1390 of epoch 1 complete. Mean batch accuracy : 0.3037769861412134\n",
            "Iteration 1400 of epoch 1 complete. Mean batch accuracy : 0.30380953153861423\n",
            "Iteration 1410 of epoch 1 complete. Mean batch accuracy : 0.30378251363728065\n",
            "Iteration 1420 of epoch 1 complete. Mean batch accuracy : 0.30352113451021656\n",
            "Iteration 1430 of epoch 1 complete. Mean batch accuracy : 0.30355478630407706\n",
            "Iteration 1440 of epoch 1 complete. Mean batch accuracy : 0.30353010032946864\n",
            "Iteration 1450 of epoch 1 complete. Mean batch accuracy : 0.30333334106823495\n",
            "Iteration 1460 of epoch 1 complete. Mean batch accuracy : 0.3030821994967657\n",
            "Iteration 1470 of epoch 1 complete. Mean batch accuracy : 0.30351474696681613\n",
            "Iteration 1480 of epoch 1 complete. Mean batch accuracy : 0.3037725302919343\n",
            "Iteration 1490 of epoch 1 complete. Mean batch accuracy : 0.30352349770349146\n",
            "Iteration 1500 of epoch 1 complete. Mean batch accuracy : 0.3033333411167065\n",
            "Iteration 1510 of epoch 1 complete. Mean batch accuracy : 0.3033664536939946\n",
            "Iteration 1520 of epoch 1 complete. Mean batch accuracy : 0.3035087797055511\n",
            "Iteration 1530 of epoch 1 complete. Mean batch accuracy : 0.3032135153956273\n",
            "Iteration 1540 of epoch 1 complete. Mean batch accuracy : 0.3029761982283422\n",
            "Iteration 1550 of epoch 1 complete. Mean batch accuracy : 0.3025806528906668\n",
            "Iteration 1560 of epoch 1 complete. Mean batch accuracy : 0.3024572726768943\n",
            "Iteration 1570 of epoch 1 complete. Mean batch accuracy : 0.30217622852629156\n",
            "Iteration 1580 of epoch 1 complete. Mean batch accuracy : 0.30210971233871164\n",
            "Iteration 1590 of epoch 1 complete. Mean batch accuracy : 0.3025157310115466\n",
            "Iteration 1600 of epoch 1 complete. Mean batch accuracy : 0.30276042443700135\n",
            "Iteration 1610 of epoch 1 complete. Mean batch accuracy : 0.3023292002979643\n",
            "Iteration 1620 of epoch 1 complete. Mean batch accuracy : 0.30262346451205235\n",
            "Iteration 1630 of epoch 1 complete. Mean batch accuracy : 0.3027096191752542\n",
            "Iteration 1640 of epoch 1 complete. Mean batch accuracy : 0.30294716217804973\n",
            "Iteration 1650 of epoch 1 complete. Mean batch accuracy : 0.3032323309643702\n",
            "Iteration 1660 of epoch 1 complete. Mean batch accuracy : 0.3032630599406828\n",
            "Iteration 1670 of epoch 1 complete. Mean batch accuracy : 0.3035928221162922\n",
            "Iteration 1680 of epoch 1 complete. Mean batch accuracy : 0.3037698490146015\n",
            "Iteration 1690 of epoch 1 complete. Mean batch accuracy : 0.3037968519347659\n",
            "Iteration 1700 of epoch 1 complete. Mean batch accuracy : 0.30372549796367393\n",
            "Iteration 1710 of epoch 1 complete. Mean batch accuracy : 0.30399610913026404\n",
            "Iteration 1720 of epoch 1 complete. Mean batch accuracy : 0.3034399302156512\n",
            "Iteration 1730 of epoch 1 complete. Mean batch accuracy : 0.30317919846904073\n",
            "Iteration 1740 of epoch 1 complete. Mean batch accuracy : 0.30320881996517896\n",
            "Iteration 1750 of epoch 1 complete. Mean batch accuracy : 0.3031428648403713\n",
            "Iteration 1760 of epoch 1 complete. Mean batch accuracy : 0.3030776592140848\n",
            "Iteration 1770 of epoch 1 complete. Mean batch accuracy : 0.30315443329555164\n",
            "Iteration 1780 of epoch 1 complete. Mean batch accuracy : 0.303230344772004\n",
            "Iteration 1790 of epoch 1 complete. Mean batch accuracy : 0.30381751237801335\n",
            "Iteration 1800 of epoch 1 complete. Mean batch accuracy : 0.3036574151242773\n",
            "Iteration 1810 of epoch 1 complete. Mean batch accuracy : 0.3034070058628011\n",
            "Iteration 1820 of epoch 1 complete. Mean batch accuracy : 0.3033424985441533\n",
            "Iteration 1830 of epoch 1 complete. Mean batch accuracy : 0.3030510095054986\n",
            "Iteration 1840 of epoch 1 complete. Mean batch accuracy : 0.30321558739828025\n",
            "Iteration 1850 of epoch 1 complete. Mean batch accuracy : 0.303063070745887\n",
            "Iteration 1860 of epoch 1 complete. Mean batch accuracy : 0.30353943420754326\n",
            "Iteration 1870 of epoch 1 complete. Mean batch accuracy : 0.30347594352249796\n",
            "Iteration 1880 of epoch 1 complete. Mean batch accuracy : 0.3032801495349788\n",
            "Iteration 1890 of epoch 1 complete. Mean batch accuracy : 0.30321870257772465\n",
            "Iteration 1900 of epoch 1 complete. Mean batch accuracy : 0.3032456217196427\n",
            "Iteration 1910 of epoch 1 complete. Mean batch accuracy : 0.3028795888169586\n",
            "Iteration 1920 of epoch 1 complete. Mean batch accuracy : 0.3031250076640087\n",
            "Iteration 1930 of epoch 1 complete. Mean batch accuracy : 0.30315199384726393\n",
            "Iteration 1940 of epoch 1 complete. Mean batch accuracy : 0.3033934784688286\n",
            "Iteration 1950 of epoch 1 complete. Mean batch accuracy : 0.30341881108589663\n",
            "Iteration 1960 of epoch 1 complete. Mean batch accuracy : 0.3031887831698571\n",
            "Iteration 1970 of epoch 1 complete. Mean batch accuracy : 0.30308799413481946\n",
            "Iteration 1980 of epoch 1 complete. Mean batch accuracy : 0.30332492351757756\n",
            "Iteration 1990 of epoch 1 complete. Mean batch accuracy : 0.3033500914635071\n",
            "Iteration 2000 of epoch 1 complete. Mean batch accuracy : 0.3035000077150762\n",
            "Iteration 2010 of epoch 1 complete. Mean batch accuracy : 0.3032338385372909\n",
            "Iteration 2020 of epoch 1 complete. Mean batch accuracy : 0.303094067081514\n",
            "Iteration 2030 of epoch 1 complete. Mean batch accuracy : 0.303160927220812\n",
            "Iteration 2040 of epoch 1 complete. Mean batch accuracy : 0.3026960861010879\n",
            "Iteration 2050 of epoch 1 complete. Mean batch accuracy : 0.3026422841011024\n",
            "Iteration 2060 of epoch 1 complete. Mean batch accuracy : 0.3029126290340447\n",
            "Iteration 2070 of epoch 1 complete. Mean batch accuracy : 0.3030998466671377\n",
            "Iteration 2080 of epoch 1 complete. Mean batch accuracy : 0.3031650718014974\n",
            "Iteration 2090 of epoch 1 complete. Mean batch accuracy : 0.3031100555505764\n",
            "Iteration 2100 of epoch 1 complete. Mean batch accuracy : 0.30281746800456727\n",
            "Iteration 2110 of epoch 1 complete. Mean batch accuracy : 0.30304108195324647\n",
            "Iteration 2120 of epoch 1 complete. Mean batch accuracy : 0.3029088127409231\n",
            "Iteration 2130 of epoch 1 complete. Mean batch accuracy : 0.3028560327427208\n",
            "Iteration 2140 of epoch 1 complete. Mean batch accuracy : 0.3028037460155298\n",
            "Iteration 2150 of epoch 1 complete. Mean batch accuracy : 0.3025969069087228\n",
            "Iteration 2160 of epoch 1 complete. Mean batch accuracy : 0.30231482249995073\n",
            "Iteration 2170 of epoch 1 complete. Mean batch accuracy : 0.30203533793374693\n",
            "Iteration 2180 of epoch 1 complete. Mean batch accuracy : 0.30179664375380094\n",
            "Iteration 2190 of epoch 1 complete. Mean batch accuracy : 0.3019025952032168\n",
            "Iteration 2200 of epoch 1 complete. Mean batch accuracy : 0.3019318258830092\n",
            "Iteration 2210 of epoch 1 complete. Mean batch accuracy : 0.30184766984633193\n",
            "Iteration 2220 of epoch 1 complete. Mean batch accuracy : 0.30150150919148516\n",
            "Iteration 2230 of epoch 1 complete. Mean batch accuracy : 0.3019058373075964\n",
            "Iteration 2240 of epoch 1 complete. Mean batch accuracy : 0.30159971008542924\n",
            "Iteration 2250 of epoch 1 complete. Mean batch accuracy : 0.30140741510192554\n",
            "Iteration 2260 of epoch 1 complete. Mean batch accuracy : 0.301401187631145\n",
            "Iteration 2270 of epoch 1 complete. Mean batch accuracy : 0.3012481721618627\n",
            "Iteration 2280 of epoch 1 complete. Mean batch accuracy : 0.30120614804654267\n",
            "Iteration 2290 of epoch 1 complete. Mean batch accuracy : 0.3014556117941459\n",
            "Iteration 2300 of epoch 1 complete. Mean batch accuracy : 0.30163044252473376\n",
            "Iteration 2310 of epoch 1 complete. Mean batch accuracy : 0.3014790842124136\n",
            "Iteration 2320 of epoch 1 complete. Mean batch accuracy : 0.3016163870467451\n",
            "Iteration 2330 of epoch 1 complete. Mean batch accuracy : 0.30157368441188287\n",
            "Iteration 2340 of epoch 1 complete. Mean batch accuracy : 0.3016737969066852\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-da53e93e62e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train(camembert_clf, criterion, opti, train_loader, val_loader,\n\u001b[0;32m----> 2\u001b[0;31m       max_eps=5, gpu=True, print_every=10, validate_every=1)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-56-3ad5b884c408>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, opti, train_loader, val_loader, max_eps, gpu, print_every, validate_every, break_training_after)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0;31m# optimization step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mopti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_RNTqu_9hYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}