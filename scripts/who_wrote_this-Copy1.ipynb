{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/meteve/NLP_project/blob/master/scripts/who_wrote_this.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sw6VsDhUun5n"
   },
   "source": [
    "# Who wrote this : a framework for French novelist identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "liRJXYg6Cv2u",
    "outputId": "b6dd62e8-4285-4764-a1fc-e70aadd1ac5d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import unidecode\n",
    "import urllib\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iPykVwoCzxbu"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3i0eCW7-DDDU"
   },
   "outputs": [],
   "source": [
    "# Import train data\n",
    "train_df = pd.read_csv('../data/corpus_train_features_NER.csv', index_col=0)\n",
    "X_train_ner = train_df['paragraph_ner'].values\n",
    "X_train = train_df['paragraph'].values\n",
    "y_labels_train = train_df['author'].values\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fPnN3eBy3slZ"
   },
   "outputs": [],
   "source": [
    "# Import test data\n",
    "test_df = pd.read_csv('../data/corpus_test_NER.csv', index_col=0)\n",
    "X_test_ner = test_df['paragraph_ner'].values\n",
    "X_test = test_df['paragraph'].values\n",
    "y_labels_test = test_df['author'].values\n",
    "y_test = le.transform(y_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avg1FM3GQ4dw"
   },
   "outputs": [],
   "source": [
    "def import_stopwords():\n",
    "    \"\"\"Download and import French stopwords.\"\"\"\n",
    "    URL = 'https://raw.githubusercontent.com/stopwords-iso/stopwords-fr/master/stopwords-fr.txt'\n",
    "    response = urllib.request.urlopen(URL)\n",
    "    stopwords = response.read().decode('utf-8').splitlines()\n",
    "    stopwords = [unidecode.unidecode(x) for x in stopwords]\n",
    "    stopwords.append('quelqu') # Make stopwords consistent with tokenization\n",
    "    return stopwords\n",
    "\n",
    "# Import french stopwords\n",
    "#with open('models/stopwords-fr.txt') as f:\n",
    " #   stopwords = f.read().splitlines()\n",
    "stopwords = import_stopwords()\n",
    "stopwords = [unidecode.unidecode(x) for x in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bSOiEE20z1o4"
   },
   "source": [
    "## Baseline TF-IDF model with and without NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOwk-ES44rEC"
   },
   "outputs": [],
   "source": [
    "# ML pipeline : TF-IDF + SVM classifier\n",
    "\n",
    "tfidf_vecto = TfidfVectorizer()\n",
    "clf = LinearSVC()\n",
    "\n",
    "tfidf_pipeline = Pipeline([\n",
    "                           ('tf-idf', tfidf_vecto),\n",
    "                           ('SVC', clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pipeline = tfidf_pipeline.fit(X_train, y_train)\n",
    "tfidf_pipeline_ner = tfidf_pipeline.fit(X_train_ner, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare train and test scores with and without NER procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on train set with TF-IDF : 0.972\n",
      "F1 score with NER procedure on train set with TF-IDF : 0.973\n"
     ]
    }
   ],
   "source": [
    "# Compute predictions on train and train score\n",
    "y_train_pred = tfidf_pipeline.predict(X_train)\n",
    "tfidf_test_score = f1_score(y_train, y_train_pred, average='micro')\n",
    "\n",
    "y_train_pred_ner = tfidf_pipeline_ner.predict(X_train_ner)\n",
    "tfidf_test_score_ner = f1_score(y_train, y_train_pred_ner, average='micro')\n",
    "\n",
    "print('F1 score on train set with TF-IDF :', \n",
    "      tfidf_test_score.round(3))\n",
    "print('F1 score with NER procedure on train set with TF-IDF :', \n",
    "      tfidf_test_score_ner.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WdyNC-ok3qxy",
    "outputId": "bb0a47bc-7951-498d-ac05-6f18b023a34d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on test set with TF-IDF : 0.501\n",
      "F1 score with NER procedure on test set with TF-IDF : 0.487\n"
     ]
    }
   ],
   "source": [
    "# Compute predictions and test score\n",
    "y_pred = tfidf_pipeline.predict(X_test)\n",
    "tfidf_test_score = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "y_pred_ner = tfidf_pipeline_ner.predict(X_test_ner)\n",
    "tfidf_test_score_ner = f1_score(y_test, y_pred_ner, average='micro')\n",
    "\n",
    "print('F1 score on test set with TF-IDF :', \n",
    "      tfidf_test_score.round(3))\n",
    "print('F1 score with NER procedure on test set with TF-IDF :', \n",
    "      tfidf_test_score_ner.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conf_matrix_df(y_true, y_pred):\n",
    "    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "    conf_matrix = pd.DataFrame(conf_matrix, columns=list(le.classes_), index=list(le.classes_))\n",
    "    \n",
    "    row_percentages = []\n",
    "    for i in range(0,10):\n",
    "        perc = round(conf_matrix.iloc[i,i]/sum(conf_matrix.iloc[0:10,i]), 3)\n",
    "        row_percentages.append(perc)\n",
    "    \n",
    "    row_percentages = pd.DataFrame([row_percentages], columns=list(le.classes_))\n",
    "    \n",
    "    col_percentages = []\n",
    "    for i in range(0,10):\n",
    "        perc = round(conf_matrix.iloc[i,i]/sum(conf_matrix.iloc[i,0:10]), 3)\n",
    "        col_percentages.append(perc)\n",
    "    col_percentages.append(None)\n",
    "        \n",
    "    \n",
    "    conf_matrix = conf_matrix.append(row_percentages)\n",
    "    conf_matrix = conf_matrix.rename(index={0: 'PRECISION'})\n",
    "    conf_matrix['RECALL'] = col_percentages\n",
    "\n",
    "    \n",
    "    return(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_train = get_conf_matrix_df(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_test = get_conf_matrix_df(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balzac</th>\n",
       "      <th>Daudet</th>\n",
       "      <th>Dumas</th>\n",
       "      <th>Flaubert</th>\n",
       "      <th>Hugo</th>\n",
       "      <th>Maupassant</th>\n",
       "      <th>Stendhal</th>\n",
       "      <th>Verne</th>\n",
       "      <th>Vigny</th>\n",
       "      <th>Zola</th>\n",
       "      <th>RECALL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Balzac</td>\n",
       "      <td>2972.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>74.000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>11.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Daudet</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1637.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>9.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Dumas</td>\n",
       "      <td>21.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>13990.000</td>\n",
       "      <td>11.00</td>\n",
       "      <td>51.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>48.00</td>\n",
       "      <td>12.000</td>\n",
       "      <td>21.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>0.987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Flaubert</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>3472.00</td>\n",
       "      <td>19.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Hugo</td>\n",
       "      <td>12.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>117.000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>6169.000</td>\n",
       "      <td>16.000</td>\n",
       "      <td>8.00</td>\n",
       "      <td>4.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>0.973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Maupassant</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.000</td>\n",
       "      <td>70.000</td>\n",
       "      <td>5.00</td>\n",
       "      <td>16.000</td>\n",
       "      <td>3347.000</td>\n",
       "      <td>17.00</td>\n",
       "      <td>2.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>0.962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Stendhal</td>\n",
       "      <td>8.00</td>\n",
       "      <td>2.000</td>\n",
       "      <td>96.000</td>\n",
       "      <td>6.00</td>\n",
       "      <td>12.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>5817.00</td>\n",
       "      <td>2.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Verne</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>38.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4114.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Vigny</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>296.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1613.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Zola</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>36.000</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>8.00</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2732.000</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>PRECISION</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.984</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Balzac    Daudet      Dumas  Flaubert      Hugo  Maupassant  \\\n",
       "Balzac      2972.00     0.000     74.000      2.00    11.000       2.000   \n",
       "Daudet         5.00  1637.000     30.000      2.00     9.000       1.000   \n",
       "Dumas         21.00     1.000  13990.000     11.00    51.000       9.000   \n",
       "Flaubert       4.00     1.000     59.000   3472.00    19.000      10.000   \n",
       "Hugo          12.00     0.000    117.000      2.00  6169.000      16.000   \n",
       "Maupassant     4.00     4.000     70.000      5.00    16.000    3347.000   \n",
       "Stendhal       8.00     2.000     96.000      6.00    12.000       7.000   \n",
       "Verne          0.00     1.000     38.000      0.00     4.000       3.000   \n",
       "Vigny          3.00     1.000    296.000      0.00    14.000       3.000   \n",
       "Zola           3.00     1.000     36.000      6.00     6.000       6.000   \n",
       "PRECISION      0.98     0.993      0.945      0.99     0.977       0.983   \n",
       "\n",
       "            Stendhal     Verne     Vigny      Zola  RECALL  \n",
       "Balzac         10.00     1.000     4.000     3.000   0.965  \n",
       "Daudet          2.00     0.000     1.000     4.000   0.968  \n",
       "Dumas          48.00    12.000    21.000     8.000   0.987  \n",
       "Flaubert       10.00     0.000     1.000     3.000   0.970  \n",
       "Hugo            8.00     4.000     1.000     8.000   0.973  \n",
       "Maupassant     17.00     2.000     3.000    13.000   0.962  \n",
       "Stendhal     5817.00     2.000     3.000     3.000   0.977  \n",
       "Verne           1.00  4114.000     0.000     1.000   0.988  \n",
       "Vigny          13.00     0.000  1613.000     2.000   0.829  \n",
       "Zola            8.00     2.000     0.000  2732.000   0.976  \n",
       "PRECISION       0.98     0.994     0.979     0.984     NaN  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 2, ..., 4, 9, 9])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 9, 9, 9])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(le.classes_).index('Daudet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = tfidf_pipeline.named_steps['tf-idf']\n",
    "Xtr = vec.fit_transform(X_train)\n",
    "features = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Xtr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below show the n highest scoring words of a paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_feats_in_doc(Xtr, features, row_id, top_n=25):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>commentaire</td>\n",
       "      <td>0.286708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>compère</td>\n",
       "      <td>0.273435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>rasoir</td>\n",
       "      <td>0.267134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>oblique</td>\n",
       "      <td>0.243613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>feignit</td>\n",
       "      <td>0.242411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>abaissa</td>\n",
       "      <td>0.239085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>grisette</td>\n",
       "      <td>0.237067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>profil</td>\n",
       "      <td>0.220577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>suzanne</td>\n",
       "      <td>0.200200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>posa</td>\n",
       "      <td>0.191618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature     tfidf\n",
       "0  commentaire  0.286708\n",
       "1      compère  0.273435\n",
       "2       rasoir  0.267134\n",
       "3      oblique  0.243613\n",
       "4      feignit  0.242411\n",
       "5      abaissa  0.239085\n",
       "6     grisette  0.237067\n",
       "7       profil  0.220577\n",
       "8      suzanne  0.200200\n",
       "9         posa  0.191618"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_feats_in_doc(Xtr, features, 7, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below computes the average tf-idf score of all words across a number of documents (in this case all documents), i.e. the average per column of a tf-idf matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_mean_feats(author, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    Xtr = train_df['paragraph'][train_df['author']==author].values\n",
    "    vec = tfidf_pipeline.named_steps['tf-idf']\n",
    "    Xtr = vec.fit_transform(X_train)\n",
    "    features = vec.get_feature_names()\n",
    "    \n",
    "    D = Xtr.toarray()\n",
    "    #D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vec_pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-bc322d89044a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtop_mean_feats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauthor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Zola'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-53-a0a519664fab>\u001b[0m in \u001b[0;36mtop_mean_feats\u001b[1;34m(author, min_tfidf, top_n)\u001b[0m\n\u001b[0;32m      3\u001b[0m         indentified by indices in grp_ids. '''\n\u001b[0;32m      4\u001b[0m     \u001b[0mXtr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'paragraph'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'author'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mauthor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mXtr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vec'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vec_pipe' is not defined"
     ]
    }
   ],
   "source": [
    "top_mean_feats(author='Zola')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g88tOQuBz792"
   },
   "source": [
    "## FastText pre-trained embeddings + averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_KlP_Iw2oth4"
   },
   "outputs": [],
   "source": [
    "# Use sklearn preprocessing pipeline to ensure results comparability\n",
    "preprocessor = tfidf_vecto.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vAYXl07XADFH"
   },
   "outputs": [],
   "source": [
    "# Import Fasttext French word vectors\n",
    "fasttext = FastText.load_fasttext_format('models/fasttext.fr.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r151opCRr8B1"
   },
   "outputs": [],
   "source": [
    "def text_to_wv_fasttext(text):\n",
    "    \"\"\"Compute average of FastText's word vectors for a given text.\"\"\"\n",
    "    if text:\n",
    "        tokens = preprocessor(text)\n",
    "        wv_mat = np.zeros((len(tokens), fasttext.vector_size))\n",
    "        for i, tok in enumerate(tokens):\n",
    "            try:\n",
    "                wv_mat[i] = fasttext.wv[tok]\n",
    "            except KeyError:\n",
    "                pass\n",
    "        text_vec = wv_mat.mean(axis=0)\n",
    "    else:\n",
    "        text_vec = np.zeros(fasttext.vector_size)\n",
    "    return text_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PRfHhI-LuzPm"
   },
   "outputs": [],
   "source": [
    "def preprocess_corpus_fasttext(corpus):\n",
    "    \"\"\"Parallelize preprocessing and document vectors computation.\"\"\"\n",
    "    with Pool(N_CORES) as p:\n",
    "        corpus_prepro = p.map(text_to_wv_fasttext, list(corpus))\n",
    "    return np.array(corpus_prepro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZV7aqaByn1B3"
   },
   "outputs": [],
   "source": [
    "class TextToWV(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Enable to use preprocessing function in a sklearn pipeline.\"\"\"\n",
    "    def __init__(self, preprocessor):\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return(self)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.preprocessor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sg3YBbgJ0ZPm"
   },
   "outputs": [],
   "source": [
    "fasttext_pipeline = Pipeline([\n",
    "                              ('fasttext_average', TextToWV(preprocess_corpus_fasttext)),\n",
    "                              ('SVC', clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTvo5xwMob3g"
   },
   "outputs": [],
   "source": [
    "# Preprocessing + training\n",
    "fasttext_pipeline = fasttext_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zIVGLvOm0pY_",
    "outputId": "08e42ad8-6110-4d1e-98a2-886c4d17a634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on test set with pre-trained FastText + averaging : 0.37\n"
     ]
    }
   ],
   "source": [
    "# Compute predictions and test score\n",
    "y_pred = fasttext_pipeline.predict(X_test)\n",
    "fasttext_test_score = f1_score(y_test, y_pred, average='micro')\n",
    "print('F1 score on test set with pre-trained FastText + averaging :',\n",
    "      fasttext_test_score.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bUqdN5M90AQV"
   },
   "source": [
    "## CamemBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "1AzW5DKbqv2C",
    "outputId": "3c103263-ea8a-43ee-d0f4-0f59a78ac7f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading archive file models/camembert.v0\n",
      "| dictionary: 32004 types\n"
     ]
    }
   ],
   "source": [
    "# Import Camembert model\n",
    "camembert = CamembertModel.from_pretrained('models/camembert.v0')\n",
    "camembert.eval()\n",
    "CAMEMBERT_WV_SIZE = camembert.extract_features(camembert.encode('test string')).shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "t6JyQxjTjl0j",
    "outputId": "c2668615-44c9-41bd-dc9b-7245f4c05e20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2334"
      ]
     },
     "execution_count": 159,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [camembert.encode(x) for x in X_train[:100]]\n",
    "max([len(x) for x in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xc4J1jNLtLYM"
   },
   "outputs": [],
   "source": [
    "def text_to_wv_camembert(text):\n",
    "    \"\"\"Compute average of CamemBERT's word vectors for a given text.\"\"\"\n",
    "    if text:\n",
    "        sentences = text.split('.')\n",
    "        sentences = [s for s in sentences if s]\n",
    "        tokens_ind = camembert.encode(*sentences)\n",
    "        text_features = camembert.extract_features(tokens_ind)\n",
    "        text_vec = text_features.squeeze(0).detach().numpy().mean(axis=0)\n",
    "    else:\n",
    "        text_vec = np.zeros(CAMEMBERT_WV_SIZE)\n",
    "    return text_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DFhXaAphzz0a"
   },
   "outputs": [],
   "source": [
    "def preprocess_corpus_camembert(corpus):\n",
    "    \"\"\"\"\"\"\n",
    "    corpus_prepro = []\n",
    "    for i, text in enumerate(corpus):\n",
    "        corpus_prepro.append(text_to_wv_camembert(text))\n",
    "        print(i)\n",
    "    return np.array(corpus_prepro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3dhGEp0ucLe"
   },
   "outputs": [],
   "source": [
    "camembert_pipeline = Pipeline([\n",
    "                              ('camembert_average', TextToWV(preprocess_corpus_camembert)),\n",
    "                              ('SVC', clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "asoCS8K6vOOz",
    "outputId": "ea946a7e-7336-440f-e0f6-6f13f82a5330"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-fdc44ffcafb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcamembert_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcamembert_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \"\"\"\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-d56f9c0b65bc>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-139-b1198c481327>\u001b[0m in \u001b[0;36mpreprocess_corpus_camembert\u001b[0;34m(corpus)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcorpus_prepro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mcorpus_prepro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_to_wv_camembert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_prepro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-d469bf51fd15>\u001b[0m in \u001b[0;36mtext_to_wv_camembert\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtokens_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcamembert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcamembert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mtext_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fairseq/models/roberta/hub_interface.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, tokens, return_all_hiddens)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             raise ValueError('tokens exceeds maximum length: {} > {}'.format(\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             ))\n\u001b[1;32m     84\u001b[0m         features, extra = self.model(\n",
      "\u001b[0;31mValueError\u001b[0m: tokens exceeds maximum length: 1321 > 512"
     ]
    }
   ],
   "source": [
    "# Preprocessing + training\n",
    "camembert_pipeline = camembert_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R7VZVZsbvUwv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPMwQA8Uz0hjIDrnyygcMC2",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "who-wrote-this.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
