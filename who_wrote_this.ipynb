{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sw6VsDhUun5n"
   },
   "source": [
    "# Who wrote this : a framework for French novelist identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "3rR8SIQ_ks-E",
    "outputId": "ed315b9a-83fc-4c74-d15d-ed934b6cd5b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
      "\r",
      "\u001b[K     |█▍                              | 10kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 20kB 1.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 30kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 40kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 51kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 61kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 71kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 81kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 92kB 3.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 102kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 112kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 122kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 133kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 143kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 153kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 163kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 174kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 184kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 194kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 204kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 215kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 225kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 235kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 245kB 2.8MB/s \n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.1.1\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.6.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.27)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.27 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.27)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "liRJXYg6Cv2u"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import unidecode\n",
    "import urllib.request\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec, FastText\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WUk_HtdMolVp"
   },
   "outputs": [],
   "source": [
    "# Number of available cores for parallel computing\n",
    "N_CORES = cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iPykVwoCzxbu"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "is9UbtTqWa9L",
    "outputId": "32381300-d248-420b-c614-033319abcb58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-27 15:42:47--  https://raw.githubusercontent.com/meteve/NLP_project/master/data/corpus_train.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18165744 (17M) [text/plain]\n",
      "Saving to: ‘corpus_train.csv.1’\n",
      "\n",
      "\r",
      "corpus_train.csv.1    0%[                    ]       0  --.-KB/s               \r",
      "corpus_train.csv.1  100%[===================>]  17.32M  98.8MB/s    in 0.2s    \n",
      "\n",
      "2020-03-27 15:42:48 (98.8 MB/s) - ‘corpus_train.csv.1’ saved [18165744/18165744]\n",
      "\n",
      "--2020-03-27 15:42:49--  https://raw.githubusercontent.com/meteve/NLP_project/master/data/corpus_test.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6894810 (6.6M) [text/plain]\n",
      "Saving to: ‘corpus_test.csv.1’\n",
      "\n",
      "corpus_test.csv.1   100%[===================>]   6.58M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2020-03-27 15:42:49 (56.9 MB/s) - ‘corpus_test.csv.1’ saved [6894810/6894810]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download data from the GitHub repository\n",
    "!wget https://raw.githubusercontent.com/meteve/NLP_project/master/data/corpus_train.csv\n",
    "!wget https://raw.githubusercontent.com/meteve/NLP_project/master/data/corpus_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3i0eCW7-DDDU"
   },
   "outputs": [],
   "source": [
    "# Import train data\n",
    "train_df = pd.read_csv('corpus_train.csv', sep='|')\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True) # Shuffle\n",
    "X_train = train_df['paragraph'].values\n",
    "y_labels_train = train_df['author'].values\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_labels_train)\n",
    "N_CLASSES = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fPnN3eBy3slZ"
   },
   "outputs": [],
   "source": [
    "# Import test data and build validation dataset\n",
    "test_df = pd.read_csv('corpus_test.csv', sep='|')\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True) # Shuffle\n",
    "X_val, X_test, y_val, y_test = train_test_split(test_df['paragraph'].values,\n",
    "                                                test_df['author'].values,\n",
    "                                                test_size=0.5, random_state=42)\n",
    "y_val = le.transform(y_val)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "Pk7Dfp3JKzTj",
    "outputId": "7035ed85-833c-4b32-cb04-5384c0e747cb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-- J'avais froid, répondit le duc, et j'attisa...</td>\n",
       "      <td>Dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ces deux mots nous rejetèrent dans un silence ...</td>\n",
       "      <td>Balzac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--Et il vous a dit, interrompit l'évêque en so...</td>\n",
       "      <td>Hugo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A six heures, le jour se fit subitement, avec ...</td>\n",
       "      <td>Verne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Il s'avança vers les cinq qui lui souriaient, ...</td>\n",
       "      <td>Hugo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           paragraph  author\n",
       "0  -- J'avais froid, répondit le duc, et j'attisa...   Dumas\n",
       "1  Ces deux mots nous rejetèrent dans un silence ...  Balzac\n",
       "2  --Et il vous a dit, interrompit l'évêque en so...    Hugo\n",
       "3  A six heures, le jour se fit subitement, avec ...   Verne\n",
       "4  Il s'avança vers les cinq qui lui souriaient, ...    Hugo"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1pc4i1aPKJqv"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bSOiEE20z1o4"
   },
   "source": [
    "### Baseline : TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ENoBUodrpxwM"
   },
   "outputs": [],
   "source": [
    "URL = 'https://raw.githubusercontent.com/stopwords-iso/stopwords-fr/master/stopwords-fr.txt'\n",
    "response = urllib.request.urlopen(URL)\n",
    "stopwords = response.read().decode('utf-8').splitlines()\n",
    "stopwords = [unidecode.unidecode(x) for x in stopwords]\n",
    "stopwords.append('quelqu') # Make stopwords consistent with scikit tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOwk-ES44rEC"
   },
   "outputs": [],
   "source": [
    "# ML pipeline : TF-IDF + SVM classifier\n",
    "\n",
    "tfidf_vecto = TfidfVectorizer(stop_words=stopwords)\n",
    "clf = LogisticRegression(max_iter=10000)\n",
    "\n",
    "tfidf_pipeline = Pipeline([\n",
    "                           ('tf-idf', tfidf_vecto),\n",
    "                           ('clf', clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_KlP_Iw2oth4"
   },
   "outputs": [],
   "source": [
    "# Keep sklearn preprocessing pipeline for later\n",
    "preprocessor = tfidf_vecto.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ML1EXEK2kA4"
   },
   "outputs": [],
   "source": [
    "# Preprocessing + training\n",
    "tfidf_pipeline = tfidf_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WdyNC-ok3qxy",
    "outputId": "469daacf-e312-4323-87f8-3eca6483be23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on validation set with TF-IDF : 0.48\n"
     ]
    }
   ],
   "source": [
    "# Compute predictions and validation score\n",
    "y_val_pred_tfidf = tfidf_pipeline.predict(X_val)\n",
    "tfidf_val_score = f1_score(y_val, y_val_pred_tfidf, average='micro')\n",
    "print('F1 score on validation set with TF-IDF :', \n",
    "      tfidf_val_score.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g88tOQuBz792"
   },
   "source": [
    "### FastText (averaging of pre-trained word vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "u9E444_cXwAy",
    "outputId": "7f8d5307-44e0-4c2f-d8bc-af464276730f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-27 14:23:02--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.bin.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 2606:4700:10::6816:4a8e, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4496886212 (4.2G) [application/octet-stream]\n",
      "Saving to: ‘cc.fr.300.bin.gz’\n",
      "\n",
      "cc.fr.300.bin.gz    100%[===================>]   4.19G  21.5MB/s    in 3m 1s   \n",
      "\n",
      "2020-03-27 14:26:04 (23.7 MB/s) - ‘cc.fr.300.bin.gz’ saved [4496886212/4496886212]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download and extract FastText French word vectors\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.bin.gz\n",
    "!gunzip cc.fr.300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vAYXl07XADFH"
   },
   "outputs": [],
   "source": [
    "# Import Fasttext French word vectors\n",
    "fasttext = FastText.load_fasttext_format('cc.fr.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r151opCRr8B1"
   },
   "outputs": [],
   "source": [
    "def text_to_wv_fasttext(text):\n",
    "    \"\"\"Compute average of FastText's word vectors for a given text.\"\"\"\n",
    "    if text:\n",
    "        tokens = preprocessor(text)\n",
    "        wv_mat = np.zeros((len(tokens), fasttext.vector_size))\n",
    "        for i, tok in enumerate(tokens):\n",
    "            try:\n",
    "                wv_mat[i] = fasttext.wv[tok]\n",
    "            except KeyError:\n",
    "                pass\n",
    "        text_vec = wv_mat.mean(axis=0)\n",
    "    else:\n",
    "        text_vec = np.zeros(fasttext.vector_size)\n",
    "    return text_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PRfHhI-LuzPm"
   },
   "outputs": [],
   "source": [
    "def preprocess_corpus_fasttext(corpus):\n",
    "    \"\"\"Parallelize preprocessing and document vectors computation.\"\"\"\n",
    "    with Pool(N_CORES) as p:\n",
    "        corpus_prepro = p.map(text_to_wv_fasttext, list(corpus))\n",
    "    return np.array(corpus_prepro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZV7aqaByn1B3"
   },
   "outputs": [],
   "source": [
    "class TextToWV(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Enable to use preprocessing function in a sklearn pipeline.\"\"\"\n",
    "    def __init__(self, preprocessor):\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return(self)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.preprocessor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sg3YBbgJ0ZPm"
   },
   "outputs": [],
   "source": [
    "# Prediction pipeline\n",
    "clf = LogisticRegression(max_iter=10000)\n",
    "fasttext_pipeline = Pipeline([\n",
    "                              ('fasttext_average', TextToWV(preprocess_corpus_fasttext)),\n",
    "                              ('clf', clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTvo5xwMob3g"
   },
   "outputs": [],
   "source": [
    "# Preprocessing + training\n",
    "fasttext_pipeline = fasttext_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zIVGLvOm0pY_",
    "outputId": "bacc522c-3acb-4bf5-ba3c-573b8af4d501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on validation set : 0.35\n"
     ]
    }
   ],
   "source": [
    "# Compute predictions and test score\n",
    "y_val_pred_fasttext = fasttext_pipeline.predict(X_val)\n",
    "val_score_fasttext = f1_score(y_val, y_val_pred_fasttext, average='micro')\n",
    "print('F1 score on validation set :',\n",
    "      val_score_fasttext.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kL0DTOLbvYq1"
   },
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ztQM6FneveCt"
   },
   "outputs": [],
   "source": [
    "def build_d2v_corpus(corpus, tokens_only=False):\n",
    "    \"\"\"Tokenize and build corpus as expected by Gensim Doc2Vec class.\"\"\"\n",
    "    corpus_tokenized = []\n",
    "    for i, text in enumerate(corpus):\n",
    "        tokens = preprocessor(text)\n",
    "        if tokens_only:\n",
    "            corpus_tokenized.append(tokens)\n",
    "        else:\n",
    "            corpus_tokenized.append(TaggedDocument(tokens, [i]))\n",
    "    return corpus_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "krsC1SYh5ACx"
   },
   "outputs": [],
   "source": [
    "# Format train and validation corpus as required by Doc2Vec\n",
    "corpus_train_d2v = build_d2v_corpus(X_train, tokens_only=False)\n",
    "corpus_val_d2v = build_d2v_corpus(X_val, tokens_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ryhxTy1C5wwy"
   },
   "outputs": [],
   "source": [
    "# Train Doc2Vec model\n",
    "model = Doc2Vec(vector_size=50, min_count=2, epochs=10, workers=N_CORES)\n",
    "model.build_vocab(corpus_train_d2v)\n",
    "model.train(corpus_train_d2v, total_examples=model.corpus_count, \n",
    "            epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pLoVbMK8_DI"
   },
   "outputs": [],
   "source": [
    "# Compute document vectors on train and validation sets\n",
    "X_train_d2v = np.array([model.infer_vector(doc.words) for doc in corpus_train_d2v])\n",
    "X_val_d2v = np.array([model.infer_vector(doc) for doc in corpus_val_d2v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Xe3LTThCAICg",
    "outputId": "15585eb1-4ce9-433c-8678-4b531c15a372"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on validation set : 0.39\n"
     ]
    }
   ],
   "source": [
    "# Compute predictions and validation score\n",
    "clf = LogisticRegression(max_iter=10000)\n",
    "clf.fit(X_train_d2v, y_train)\n",
    "y_val_pred_d2v = clf.predict(X_val_d2v)\n",
    "val_score_d2v = f1_score(y_val, y_val_pred_d2v, average='micro')\n",
    "print('F1 score on validation set :', \n",
    "      val_score_d2v.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bUqdN5M90AQV"
   },
   "source": [
    "### CamemBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N6W9sKtFRXbL"
   },
   "outputs": [],
   "source": [
    "class CorpusToTorchDataset(Dataset):\n",
    "    \"\"\"Convert corpus to tensors of token indices in CamemBERT vocabulary.\"\"\"\n",
    "    def __init__(self, corpus, labels, model_name, maxlen=100):\n",
    "        self.corpus = corpus\n",
    "        self.labels = labels\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Select instance\n",
    "        sentence = self.corpus[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        # Preprocess data as required by BERT models\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        bos_token = self.tokenizer.cls_token\n",
    "        eos_token = self.tokenizer.sep_token\n",
    "        pad_token = self.tokenizer.pad_token\n",
    "        # Insert CLS and SEP tokens at beginning and end of sentence\n",
    "        tokens = [bos_token] + tokens + [eos_token]\n",
    "        if len(tokens) < self.maxlen:\n",
    "            # If sentence is shorter than maxlen, pad sentence using special \n",
    "            # padding token\n",
    "            tokens = tokens + [pad_token for _ in range(self.maxlen - len(tokens))]\n",
    "        else:\n",
    "            # Cut the sentence if it is longer than maxlen\n",
    "            tokens = tokens[:self.maxlen-1] + [eos_token]\n",
    "\n",
    "        # Convert tokens to tensor of indices in CamemBERT vocabulary\n",
    "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids)\n",
    "        # Get attention mask to distinguish padding tokens from actual tokens\n",
    "        pad_token_id = self.tokenizer.convert_tokens_to_ids(pad_token)\n",
    "        attn_mask = (tokens_ids_tensor != pad_token_id).long()\n",
    "\n",
    "        return tokens_ids_tensor, attn_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDnF51VmVtHG"
   },
   "outputs": [],
   "source": [
    "# Create instances of training and validation dataloaders\n",
    "\n",
    "BERT_MODEL_NAME = 'camembert-base'\n",
    "MAXLEN = 100\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "train_set = CorpusToTorchDataset(X_train, y_train, model_name=BERT_MODEL_NAME, maxlen=100)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, num_workers=N_CORES)\n",
    "\n",
    "val_set = CorpusToTorchDataset(X_val, y_val, model_name=BERT_MODEL_NAME, maxlen=100)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, num_workers=N_CORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ydN8F8gOUI4"
   },
   "outputs": [],
   "source": [
    "class CamemBERTClassifier(nn.Module):\n",
    "    \"\"\"Perform fine-tuning and classification using CamemBERT.\"\"\"\n",
    "    def __init__(self, pretrained_model_name=BERT_MODEL_NAME):\n",
    "        super(CamemBERTClassifier, self).__init__()\n",
    "        # Load CamemBERT\n",
    "        self.encoder = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        # Add an extra dense layer to perform classification\n",
    "        self.cls_layer = nn.Linear(self.encoder.pooler.dense.out_features, N_CLASSES)\n",
    "\n",
    "    def forward(self, seq, attn_masks):\n",
    "        # Feed input to BERT model to obtain contextualized representations\n",
    "        cont_reps, _ = self.encoder(seq, attention_mask = attn_masks)\n",
    "        # Get representation of [CLS] head\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "        # Feed document representation to the classifying layer\n",
    "        logits = self.cls_layer(cls_rep)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_4A_4QjGOUP6"
   },
   "outputs": [],
   "source": [
    "# Instantiate CamemBERT classifier model\n",
    "camembert_clf = CamemBERTClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3f14mz-vhsuE"
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "criterion = CrossEntropyLoss()\n",
    "opti = Adam(camembert_clf.parameters(), lr = 3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ZUQxW1biwti"
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, opti, train_loader, val_loader, max_eps=3, \n",
    "          gpu=True, print_every=100, validate_every=1):\n",
    "    \"\"\"Train a transformer model and compute loss on validation data.\"\"\"\n",
    "    if gpu:\n",
    "        model = model.to(\"cuda\")\n",
    "    # Unfreeze weights to allow fine tuning\n",
    "    model.train() \n",
    "\n",
    "    train_loss_total = 0\n",
    "    n_batch_train = 0\n",
    "    for ep in range(max_eps):\n",
    "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
    "            # Clear gradients (avoid accumulation)\n",
    "            opti.zero_grad()  \n",
    "            # Transfer tensors to GPU\n",
    "            if gpu:\n",
    "                seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
    "            # Compute logits\n",
    "            logits = model(seq, attn_masks)\n",
    "            # Compute batch loss\n",
    "            loss = criterion(logits, labels)\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            # Optimization step\n",
    "            opti.step()\n",
    "            # Accumulate train loss over batches\n",
    "            train_loss_total += loss.item()\n",
    "            n_batch_train += 1\n",
    "\n",
    "            # Compute average loss over the last `print_every` training batches\n",
    "            if print_every is not None and (it + 1) % print_every == 0:\n",
    "                print(f'Epoch {ep+1}, batch {it+1}. Average loss over last {print_every} training batches : {train_loss_total/n_batch_train}')\n",
    "                # Reinitialize accumulators\n",
    "                train_loss_total = 0\n",
    "                n_batch_train = 0\n",
    "\n",
    "        if validate_every is not None and ep % validate_every == 0:\n",
    "            # Evaluation on the validation set\n",
    "\n",
    "            predictions_val = []\n",
    "            true_labels_val = []\n",
    "            for it, (seq, attn_masks, labels) in enumerate(val_loader):\n",
    "                if gpu:\n",
    "                    seq, attn_masks = seq.cuda(), attn_masks.cuda()\n",
    "                # Compute logits without constructing the computing graph\n",
    "                # (only needed for backprop)\n",
    "                with torch.no_grad():\n",
    "                    logits_val = model(seq, attn_masks)\n",
    "                preds_batch = torch.argmax(logits_val, 1).cpu().numpy()\n",
    "                predictions_val.extend(preds_batch)\n",
    "                true_labels_val.extend(labels.numpy())\n",
    "\n",
    "            val_f1 = f1_score(true_labels_val, predictions_val, average='micro')\n",
    "            print('------------------------------------------------------------')\n",
    "            print(\"Epoch {} complete. F1 score on validation data : {}\".format(ep+1, val_f1))\n",
    "            print('------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "6nfxNPWenHb-",
    "outputId": "ba12aef4-bd13-40e2-c88d-52a0773fb3d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 500. Average loss over last 500 training batches : 1.6164042003154755\n",
      "Epoch 1, batch 1000. Average loss over last 500 training batches : 1.0134558594226837\n",
      "Epoch 1, batch 1500. Average loss over last 500 training batches : 0.7353367038667202\n",
      "Epoch 1, batch 2000. Average loss over last 500 training batches : 0.611015151232481\n",
      "Epoch 1, batch 2500. Average loss over last 500 training batches : 0.5248016767948865\n",
      "Epoch 1, batch 3000. Average loss over last 500 training batches : 0.4910539143830538\n",
      "Epoch 1, batch 3500. Average loss over last 500 training batches : 0.4564877462014556\n",
      "------------------------------------------------------------\n",
      "Epoch 1 complete. F1 score on validation data : 0.5199483689274819\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Enforce deterministic behavior to ensure reproducibility of the results\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "train(camembert_clf, criterion, opti, train_loader, val_loader,\n",
    "      max_eps=1, gpu=True, print_every=500, validate_every=1)\n",
    "\n",
    "# Grid search (only 1 epoch each time, systematic overtraining above that) :\n",
    "# lr = 5e-5 => val_f1 = 0.49\n",
    "# lr = 3e-5 => val_f1 = 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UFNaQyUBKf6B"
   },
   "source": [
    "# Final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3tvHk02Kg4a"
   },
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "\n",
    "y_test_pred_tfidf = tfidf_pipeline.predict(X_test)\n",
    "test_score_tfidf = f1_score(y_test, y_test_pred_tfidf, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0lLVo45ILnqZ"
   },
   "outputs": [],
   "source": [
    "# FastText\n",
    "\n",
    "y_test_pred_fasttext = fasttext_pipeline.predict(X_test)\n",
    "test_score_fasttext = f1_score(y_test, y_test_pred_fasttext, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SxG0wUKGQ7PM"
   },
   "outputs": [],
   "source": [
    "# doc2vec\n",
    "\n",
    "corpus_test_d2v = build_d2v_corpus(X_test, tokens_only=True)\n",
    "X_test_d2v = np.array([model.infer_vector(doc) for doc in corpus_test_d2v])\n",
    "y_test_pred_d2v = clf.predict(X_test_d2v)\n",
    "test_score_d2v = f1_score(y_test, y_test_pred_d2v, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iTCCjA3yR0o0"
   },
   "outputs": [],
   "source": [
    "# CamemBERT\n",
    "\n",
    "# Create instance of test dataloader\n",
    "test_set = CorpusToTorchDataset(X_test, y_test, model_name=BERT_MODEL_NAME, maxlen=100)\n",
    "test_loader = DataLoader(test_set, batch_size = 12, num_workers = N_CORES)\n",
    "\n",
    "def eval(model, test_loader, gpu=True):\n",
    "    \"\"\"Compute predictions of a trained transforme model on test data.\"\"\"\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    for it, (seq, attn_masks, labels) in enumerate(test_loader):\n",
    "        if gpu:\n",
    "            seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
    "        # Compute logits without constructing the computing graph\n",
    "        # (only needed for backprop)\n",
    "        with torch.no_grad():\n",
    "            logits = model(seq, attn_masks)\n",
    "        # Compute predictions from logits and store them as arrays\n",
    "        preds_batch = torch.argmax(logits, 1).cpu().numpy()\n",
    "        predictions.extend(preds_batch)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "y_test_pred_camembert = eval(camembert_clf, test_loader)\n",
    "test_score_camembert = f1_score(y_test, y_test_pred_camembert, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "id": "H7Xu42gIhQd_",
    "outputId": "f7714a35-0e4c-410d-e5a0-c3aa6a8a9794"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.515898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CamemBERT</td>\n",
       "      <td>0.515312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.388948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FastText</td>\n",
       "      <td>0.350933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model  F1 score\n",
       "0     TF-IDF  0.515898\n",
       "3  CamemBERT  0.515312\n",
       "2    Doc2Vec  0.388948\n",
       "1   FastText  0.350933"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare performances\n",
    "models = ['TF-IDF', 'FastText', 'Doc2Vec', 'CamemBERT']\n",
    "scores = [test_score_tfidf, test_score_fasttext, test_score_d2v, test_score_camembert]\n",
    "df_scores = pd.DataFrame(zip(models, scores), columns=['Model', 'F1 score'])\n",
    "df_scores.sort_values('F1 score', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of who_wrote_this.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
