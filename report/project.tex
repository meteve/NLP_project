\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{smartdiagram}
\usepackage{url}
\usepackage{xcolor}


\aclfinalcopy

\newcommand\BibTeX{B\textsc{ib}\TeX}
\title{\textit{Who wrote this : a framework for French novelist identification} \\
Machine Learning for Natural Language Processing 2020}

\author{Romain Avouac \\
  ENSAE  \\
  \texttt{romain.avouac@ensae.fr} \\\And
  Margot Eteve \\
  ENSAE \\
  \texttt{margot.eteve@ensae.fr} \\}

\setlength{\parindent}{0cm}
\date{}

\begin{document}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
Identifying and attributing authorship of a document can be beneficial for multiple applications, such as plagiarism detection and bibliometrics. In this project, we develop a NLP pipeline which performs authorship identification using short texts from French 19th century main novelists.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Framing}

Being able to identify authorship of a document is beneficial for a wide range of applications. It can prove very valuable from an historical perspective, since archives worldwide are full of documents whose authorship is not known with certainty. Furthermore, multiple plagiarism cases in literature could be solved with such an algorithm~; for instance, the authorship of some of Moliere or Shakespeare works has been debated since the 19th century\footnote{See \href{https://fr.wikipedia.org/wiki/Paternit\%C3\%A9\_des\_\%C5\%93uvres_de_Moli\%C3\%A8re}{here} and \href{https://fr.wikipedia.org/wiki/Paternit\%C3\%A9\_des\_\%C5\%93uvres\_de\_Shakespeare}{here} for more details.}. Against that background, this project aims at developing a natural language processing (NLP) pipeline for authorship identification. Specifically, we focus on the task of identifying authorship of short texts written by French novelists based on their literary style.

\section{Experiments Protocol}

\textbf{Data.} We choose to limit our experiment to a selection of ten French novelists from the 19th century\footnote{Zola, Maupassant, Daudet, Stendhal, Balzac, Flaubert, Hugo, Dumas, Vigny and Verne.}. Several reasons motivate this choice. First, in order to develop an algorithm that actually distinguishes authors based on their writing style, we need to select authors from a similar time period. The 19th century features coherent and identifiable literary movements, whereas the 20th century literary is much more scattered for instance. Besides, the language used in 19th century books appears close enough to contemporary French, enabling the use of embeddings pre-trained on modern corpora. Finally, 19th century books are now in the public domain, and importantly directly available in digital format thanks to the \textit{Project Gutenberg}\footnote{\url{https://www.gutenberg.org/}}. For each author, three representative books have been selected : two for the training set and one for the test set. This clear separation between train and test sets ensures that we are not using a given book pecularities (e.g character or place names) for prediction. Each book has been cut into paragraphs and each paragraph has been associated its author as label. This unit of analysis has been chosen because it is large enough that it can contain substantial statistical information and yet small enough that it can be processed easily by most NLP algorithms.

\textbf{Preprocessing.} A major advantage of using data from the \textit{Project Gutenberg} is that it is highly normalized and virtually noiseless. It thus requires very little preprocessing to be directly usable in a machine learning pipeline. First, we perform a tokenization step. However, as we seek to distinguish authors based on their writing style, results can be highly sensitive to the choices made at this step. For instance, the amount of punctuation and the way it is used can be very distinctive of an author's style, yet it is generally removed by standard tokenizers. In order to ensure robustness of the results, we replicate our analysis using various tokenization heuristics. We also experiment with stemming and lemmatization techniques to determine whether normalizing data can improve performance.

\textbf{Models.} We compare the performance of several models on the classification task. Our baseline consists in a linear classifier (logistic regression) trained on the TF-IDF weights matrix. All the other approaches we leverage are based on continuous word representations. The main challenge in this case is the computation of document vectors -- paragraph vectors in our case. Since there aren't actual theoretical guidelines to do so, we employ various models and heuristics. First, we use a fully unsupervised technique : paragraph vectors are computed as a simple average of \verb|FastText| \cite{joulin2016bag} pretrained French word vectors. Then we implement \verb|doc2vec| \cite{le2014distributed}, a supervised approach designed to learn word and document vectors jointly. For these two approaches, classification is also performed by training a linear classifier on the paragraph vectors matrix. Finally, we leverage transformer models, which generally achieve state-of-the-art results in most NLP tasks. Specifically, we use the \verb|CamemBERT| model \cite{martin2019camembert}, a \verb|BERT| model pretrained on a large French corpus. The model is fine-tuned in a supervised way through a simple dense layer with softmax activation.

\textbf{Training.} All models are trained in a Google Colab environment with a CPU. Both the NLP and the classification models are fine-tuned using grid search on a validation set which consists in half the test set. The other half is used for final evaluation.

\textbf{Evaluation.}  We deal with imbalanced classes -- the number of paragraphs per book varies significantly -- so accuracy is not relevant as an evaluation metric . As there are no \textit{a priori} reason to favor either precision or recall, we use the F1 metric, which puts equal emphasis on both metrics. Since we deal with is a multi-class classification task, we have to decide on an aggregation procedure to get a synthetic quality metric. We choose the micro-averaged F1 score, which consists in computing the F1 metric globally by counting the total numbers of true positives, false negatives and false positives. We also provide a more qualitative analysis of performance by analyzing the confusion matrix. \\

\section{Results}

Results are presented in this Colab notebook\footnote{https://nlp-ensae.github.io/}.

\section{Discussion}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DATA SELECTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
.
\newpage

\bibliographystyle{plain}
\bibliography{biblio}


\end{document}